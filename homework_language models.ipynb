{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f379c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = open('2ch_corpus.txt', encoding=\"utf8\").read()\n",
    "news = open('lenta.txt', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dcb2bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "972a433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dvach = normalize(dvach)\n",
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e308ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина корпуса токсичных постов в токенах - 1858941\n",
      "Длина корпуса новостных текстов в токенах -  1505789\n"
     ]
    }
   ],
   "source": [
    "print(\"Длина корпуса токсичных постов в токенах -\", len(norm_dvach))\n",
    "print(\"Длина корпуса новостных текстов в токенах - \", len(norm_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2fe3e266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных токенов в корпусе токсичных постов - 189515\n",
      "Уникальный токенов в корпусе новостных текстов -  116302\n"
     ]
    }
   ],
   "source": [
    "print(\"Уникальных токенов в корпусе токсичных постов -\", len(set(norm_dvach)))\n",
    "print(\"Уникальный токенов в корпусе новостных текстов - \", len(set(norm_news)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "07dd06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d906af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dvach = Counter(norm_dvach)\n",
    "vocab_news = Counter(norm_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "bd635216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.030066580918921042),\n",
       " ('в', 0.02628001641794979),\n",
       " ('не', 0.02506911192985684),\n",
       " ('на', 0.015955320798239428),\n",
       " ('что', 0.014345802260534357),\n",
       " ('я', 0.011691602907246653),\n",
       " ('а', 0.011463516055646737),\n",
       " ('с', 0.011339789697467536),\n",
       " ('это', 0.009536074571489897),\n",
       " ('ты', 0.008321404498582796),\n",
       " ('как', 0.007882444897390503),\n",
       " ('у', 0.006848522895562581),\n",
       " ('но', 0.005786090037284669),\n",
       " ('так', 0.005383172462170666),\n",
       " ('по', 0.005060945990217011),\n",
       " ('то', 0.005049649235774562),\n",
       " ('все', 0.0046537248896011225),\n",
       " ('за', 0.004583792600195488),\n",
       " ('же', 0.004228751746289958),\n",
       " ('если', 0.004209385881531474)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
    "probas_dvach.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a7d7f94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', 0.04808907489694771),\n",
       " ('и', 0.0221080111489724),\n",
       " ('на', 0.018883123731146926),\n",
       " ('по', 0.012943380513471676),\n",
       " ('что', 0.011310349590812525),\n",
       " ('с', 0.01057319451795703),\n",
       " ('не', 0.008435444806676101),\n",
       " ('из', 0.005131529052211166),\n",
       " ('о', 0.00499073907433246),\n",
       " ('как', 0.0049900749706632205),\n",
       " ('к', 0.00407161959610543),\n",
       " ('за', 0.0040125143695431435),\n",
       " ('россии', 0.0036751497055696383),\n",
       " ('для', 0.003325831175549828),\n",
       " ('его', 0.003260084912295149),\n",
       " ('он', 0.0031704309169478593),\n",
       " ('от', 0.003066830744546547),\n",
       " ('сообщает', 0.003050228152815567),\n",
       " ('а', 0.0029180715226369697),\n",
       " ('также', 0.002716184007188258)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
    "probas_news.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "942fae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    for word in normalize(text):\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += (np.log(1/len(norm_dvach)))\n",
    "    \n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f4af5729",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "55ebbb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8958314050721132e-50"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_joint_proba(phrase, probas_dvach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d6cd61d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.573351371331133e-45"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_joint_proba(phrase, probas_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cc761b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\posch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "709d81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def ngrammer(tokens, n=3):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "da71fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] + ['<end>'] for text in sent_tokenize(dvach[:5000000])]\n",
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] + ['<end>'] for text in sent_tokenize(news[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e55ddd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    trigrams_dvach.update(ngrammer(sentence))\n",
    "    \n",
    "unigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    trigrams_news.update(ngrammer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ac527a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba_markov_assumption(text, word_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    for ngram in ngrammer(['<start>'] + ['<start>'] + normalize(phrase) + ['<end>'] + ['<end>']):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        if word1 in word_counts and ngram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[ngram]/word_counts[word3])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return np.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "831eadfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Эта фраза более вероятна в корпусе двача\n",
    "phrase = 'Безграмотное быдло с дубляжом, войсовером, порнографией и котикам'\n",
    "\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams_dvach, trigrams_dvach) > \\\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams_news, trigrams_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "55168708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'\n",
    "\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams_dvach, trigrams_dvach) > \\\n",
    "compute_joint_proba_markov_assumption(phrase, unigrams_news, trigrams_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1636e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from numpy import array, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "afd036fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type '<start>' not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7952/3434124270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                          len(unigrams_dvach)))\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# к матрице нужно обращаться по индексам\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# поэтому зафиксируем порядок слов в словаре и сделаем маппинг id-слово и слово-id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: data type '<start>' not understood"
     ]
    }
   ],
   "source": [
    "# матрица слова на слова (инициализируем нулями)\n",
    "matrix_dvach = lil_matrix((len(unigrams_dvach), \n",
    "                         len(unigrams_dvach)))\n",
    "\n",
    "bigrams = array(word1, word2)\n",
    "# к матрице нужно обращаться по индексам\n",
    "# поэтому зафиксируем порядок слов в словаре и сделаем маппинг id-слово и слово-id\n",
    "id2word_dvach = bigrams\n",
    "word2id_dvach = {word : i for i, word in enumerate(bigrams)}\n",
    "\n",
    "# заполняем матрицу\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    # на пересечение двух слов ставим вероятность встретить второе после первого\n",
    "    matrix_dvach[word2id_dvach[bigrams], word2id_dvach[word3]] =  (trigrams_dvach[ngram]/unigrams_dvach[word3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# то же самое для другого корпуса\n",
    "matrix_news = lil_matrix((len(unigrams_news), \n",
    "                        len(unigrams_news)))\n",
    "\n",
    "id2word_news = list(unigrams_news)\n",
    "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
    "\n",
    "\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    matrix_news[word2id_news[word1, word2], word2id_news[word3]] =  (trigrams_news[ngram]/\n",
    "                                                                     unigrams_news[word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9736bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
    "    text = []\n",
    "    current_idx = word2id[start]\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "# просто выбирать наиболее вероятное продолжение не получится\n",
    "# можете попробовать раскоментировать следующую строчку и посмотреть что получается\n",
    "# в современных языковых моделях есть специальный параметр, который\n",
    "# позволяет регулировать разнообразность/случайность генерации\n",
    "# он называется температура, чем выше температура тем ближе будет к argmax\n",
    "# чем меньше температура тем ближе к полностью рандомной генерации\n",
    "#         chosen = matrix[current_idx].argmax()\n",
    "        text.append(id2word[chosen])\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = word2id['<start>']\n",
    "        current_idx = chosen\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1c97d65b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<start>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7952/928240103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix_dvach\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word_dvach\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2id_dvach\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<end>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7952/1243917.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(matrix, id2word, word2id, n, start)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'<start>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcurrent_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '<start>'"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, word2id_dvach).replace('<end>','\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b86466e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matrix_news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7952/3487424431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix_news\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word_news\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2id_news\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<end>'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'matrix_news' is not defined"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news, word2id_news).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1647927e",
   "metadata": {},
   "source": [
    "Перплексия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25076881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef927617",
   "metadata": {},
   "source": [
    "Если нам необходимо учесть, что слова могут не встречаться в предзаданном словаре, то мы создаём т.н. \"систему открытого словаря\", добавляя тег <UNK>(неизвестный). В некоторых системах любое слово, которого нет в словаре, превращается в <UNK>, и затем его вероятность считается как для любого другого слова в тренировочном сете данных. \n",
    "\n",
    "Другой вариант - мы не берём словарь заранее, но создаём его по мере работы. В таком случае слова заменяются на <UNK> в зависимости от частоты. Но необходимо определить размер словаря заранее, чтобы определить, насколько редким должно быть слово, чтобы мы могли заменить его на <UNK>.\n",
    "\n",
    "Выбор модели <UNK> влияет на перплексию и подобные ей метрики. Низкая перплексия может получиться, например, если взять маленький словарь и приписать <UNK> большую вероятность. Поэтому перплексию стоит использовать для моделей с одинаковым словарём."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780e8a1",
   "metadata": {},
   "source": [
    "Сглаживание необходимо, чтобы учесть слова, которые не встречаются в тренировочной выборке (но которые могут появиться в тестовой). Как правило, мы отнимаем немного вероятности у самых частотных слов и добавляем неизвестным.\n",
    "\n",
    "Сглаживание Лапласа (сглаживание +1) - добавить единицу ко всем н-граммам перед расчётом вероятности. Не используется в современных н-грамных моделях, но для классификации текста всё ещё подходит.\n",
    "\n",
    "Сглаживание +к - добавить к событиям, которые пока не встречались,  не единицу, а десятичную величину (0.5, 0.05, 0.01). Но для использования этого метода нужно знать, как именно мы выбираем к.\n",
    "\n",
    "Бэк-офф и Интерполяция. Бэк-офф: если нет доказательств, что встречается триграмма, мы сужаем контекст до биграммы или даже униграммы (без доказательств не выбираем н-грамму больщого порядка). Таким образом мы \"отступаем\", пока не найдётся н-грамма с ненулевой вероятностью (т.е. которая уже встречалась в тренировочной выборке). С наиболее вероятных слов убирается немного вероятности, чобы у менее вероятных слов тоже был некоторый вес (т.к. они всё же могут встречаться)\n",
    "Интерполяция: сочетаем оценки вероятности всех н-грамм, комбинируя триграмму, биграмму и униграмму (например, с помощью линейной интерполяции).\n",
    "\n",
    "Сглаживание Кнезера-Нея - самое эффективное. Заключается в том, чтобы из каждого показателя вычесть фиксированное чилсло (можно ли таким образом перевести absolute discounting как \"абсолютное вычитание\" не уверена:)). Также при этом сглаживании учитывается контекст (если слово появлялось во множестве контекстов, тем вероятней, что оно появится в новом).\n",
    "\n",
    "Глупый бэк-офф (глупое отступление?) - не стремится к распределению вероятности. Действует по факту: если н-грамма высокого порядка не встречается, мы отступаем к н-грамме предыдущего порядка, которой соответствует фиксированный вес вне контекста. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
