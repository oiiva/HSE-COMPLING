{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4daccc9",
   "metadata": {},
   "source": [
    "# Генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from scipy.sparse import lil_matrix\n",
    "from numpy import array, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f379c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvach = open('2ch_corpus.txt', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb2bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da71fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] + ['<end>'] for text in sent_tokenize(dvach[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709d81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(0, len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e55ddd39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unigrams_dvach = Counter()\n",
    "bigrams_dvach = Counter()\n",
    "trigrams_dvach = Counter()\n",
    "\n",
    "for sentence in sentences_dvach:\n",
    "    unigrams_dvach.update(sentence)\n",
    "    bigrams_dvach.update(ngrammer(sentence, 2))\n",
    "    trigrams_dvach.update(ngrammer(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd036fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
    "                           len(unigrams_dvach)))\n",
    "\n",
    "\n",
    "id2word_dvach = list(unigrams_dvach)\n",
    "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
    "id2bigram_dvach = list(bigrams_dvach)\n",
    "bigram2id_dvach = {bigram:i for i, bigram in enumerate(id2bigram_dvach)}\n",
    "\n",
    "\n",
    "for ngram in trigrams_dvach:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = ' '.join([word1, word2])\n",
    "    matrix_dvach[bigram2id_dvach[bigram], word2id_dvach[word3]] = (trigrams_dvach[ngram]/bigrams_dvach[bigram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9736bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(matrix, id2word, bigram2id, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = bigram2id[start]\n",
    "    current_bigram = start\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word[chosen])\n",
    "        current_bigram = f'{current_bigram.split()[1]} {id2word[chosen]}'\n",
    "        \n",
    "        if id2word[chosen] == '<end>':\n",
    "            chosen = bigram2id[start]\n",
    "            current_bigram = start\n",
    "            \n",
    "        current_idx = bigram2id[current_bigram]\n",
    "    \n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e8f5849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оп насчёт пользы просмотра фильмов тарковского городские реднеки верят им и бегут обратно к тянке встретил ее в посадке потом и кол оф дунью затравим \n",
      " цыгане возьмут ракку \n",
      " лол баста недавно попал в компанию \n",
      " я вот неделю назад обновился до 1 урезали \n",
      " кризис произойдет в любом случае всё не просто так мамку анона ебал и так переполнен всяким говном а какме должны быть типа путешествие во времени в 2007 метолкорщики давно гриву сбрили \n",
      " да ему похуй ирл так и познакомилися \n",
      " n по 50000 долларов платят а тесла-кар стоит 200 баксов 12 \n",
      " поэтому я\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_dvach, id2word_dvach, bigram2id_dvach).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1647927e",
   "metadata": {},
   "source": [
    "# Перплексия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1083e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_dvach = [normalize(text) for text in sent_tokenize(dvach[-3000:])][0:39]\n",
    "text_generated = generate(matrix_dvach, id2word_dvach, bigram2id_dvach).replace('<end>', '\\n').split('\\n')[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90e45aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_proba_unigrams(text, word_probas):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for word in tokens:\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += np.log(2e-4)\n",
    "    \n",
    "    return prob, len(tokens)\n",
    "\n",
    "\n",
    "def compute_join_proba_markov_assumption_bigrams(text, word_counts, bigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for ngram in ngrammer(['<start>'] + tokens + ['<end>'], 2):\n",
    "        word1, word2 = ngram.split()\n",
    "        if word1 in word_counts and ngram in bigram_counts:\n",
    "            prob += np.log(bigram_counts[ngram]/word_counts[word1])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)\n",
    "\n",
    "\n",
    "def compute_join_proba_markov_assumption_trigrams(text, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for ngram in ngrammer(['<start>'] + tokens + ['<end>'], 3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = ' '.join([word1, word2])\n",
    "        if bigram in bigram_counts and ngram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[ngram]/bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed30f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_unigrams = [perplexity(*compute_joint_proba_unigrams(i, probas_dvach)) for i in text_generated]\n",
    "perplexity_bigrams = [perplexity(*compute_join_proba_markov_assumption_bigrams(i, unigrams_dvach, bigrams_dvach)) for i in text_generated]\n",
    "perplexity_trigrams = [perplexity(*compute_join_proba_markov_assumption_trigrams(i, bigrams_dvach, trigrams_dvach)) for i in text_generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa7aed20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перплексия униграмной модели - 5000.000000000001\n",
      "Перплексия биграмной модели - 228.3861221437613\n",
      "Перплексия триграмной модели - 3.471894402836019\n"
     ]
    }
   ],
   "source": [
    "print(f'Перплексия униграмной модели - {np.mean(perplexity_unigrams)}')\n",
    "print(f'Перплексия биграмной модели - {np.mean(perplexity_bigrams)}')\n",
    "print(f'Перплексия триграмной модели - {np.mean(perplexity_trigrams)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5b69b",
   "metadata": {},
   "source": [
    "Вывод: чем больше мы учитываем контекст, тем меньше перплексия (и тем более осмысленным получается текст)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef927617",
   "metadata": {},
   "source": [
    "Если нам необходимо учесть, что слова могут не встречаться в предзаданном словаре, то мы создаём т.н. \"систему открытого словаря\", добавляя тег <UNK>(неизвестный). В некоторых системах любое слово, которого нет в словаре, превращается в <UNK>, и затем его вероятность считается как для любого другого слова в тренировочном сете данных. \n",
    "\n",
    "Другой вариант - мы не берём словарь заранее, но создаём его по мере работы. В таком случае слова заменяются на <UNK> в зависимости от частоты. Но необходимо определить размер словаря заранее, чтобы определить, насколько редким должно быть слово, чтобы мы могли заменить его на <UNK>.\n",
    "\n",
    "Выбор модели <UNK> влияет на перплексию и подобные ей метрики. Низкая перплексия может получиться, например, если взять маленький словарь и приписать <UNK> большую вероятность. Поэтому перплексию стоит использовать для моделей с одинаковым словарём."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780e8a1",
   "metadata": {},
   "source": [
    "Сглаживание необходимо, чтобы учесть слова, которые не встречаются в тренировочной выборке (но которые могут появиться в тестовой). Как правило, мы отнимаем немного вероятности у самых частотных слов и добавляем неизвестным.\n",
    "\n",
    "Сглаживание Лапласа (сглаживание +1) - добавить единицу ко всем н-граммам перед расчётом вероятности. Не используется в современных н-грамных моделях, но для классификации текста всё ещё подходит.\n",
    "\n",
    "Сглаживание +к - добавить к событиям, которые пока не встречались,  не единицу, а десятичную величину (0.5, 0.05, 0.01). Но для использования этого метода нужно знать, как именно мы выбираем к.\n",
    "\n",
    "Бэк-офф и Интерполяция. Бэк-офф: если нет доказательств, что встречается триграмма, мы сужаем контекст до биграммы или даже униграммы (без доказательств не выбираем н-грамму больщого порядка). Таким образом мы \"отступаем\", пока не найдётся н-грамма с ненулевой вероятностью (т.е. которая уже встречалась в тренировочной выборке). С наиболее вероятных слов убирается немного вероятности, чобы у менее вероятных слов тоже был некоторый вес (т.к. они всё же могут встречаться)\n",
    "Интерполяция: сочетаем оценки вероятности всех н-грамм, комбинируя триграмму, биграмму и униграмму (например, с помощью линейной интерполяции).\n",
    "\n",
    "Сглаживание Кнезера-Нея - самое эффективное. Заключается в том, чтобы из каждого показателя вычесть фиксированное чилсло (можно ли таким образом перевести absolute discounting как \"абсолютное вычитание\" не уверена:)). Также при этом сглаживании учитывается контекст (если слово появлялось во множестве контекстов, тем вероятней, что оно появится в новом).\n",
    "\n",
    "Глупый бэк-офф (глупое отступление?) - не стремится к распределению вероятности. Действует по факту: если н-грамма высокого порядка не встречается, мы отступаем к н-грамме предыдущего порядка, которой соответствует фиксированный вес вне контекста. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
